{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "prostate-arthritis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
    "from tokenizers import Tokenizer, models\n",
    "from tokenizers import Tokenizer\n",
    "#gold standard of GPT2BPE \n",
    "en_vocab='/workspace/SVdata/gpt2bpe/gpt2-vocab.json'\n",
    "en_merge='/workspace/SVdata/gpt2bpe/gpt2-merges.txt'\n",
    "\n",
    "tokenizer_en = Tokenizer(models.BPE())\n",
    "tokenizer_en.model = models.BPE.from_file(en_vocab, en_merge) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "atlantic-inspiration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'went', 't', 'othe', 'bank', 'th', 'ism', 'or', 'ning', ',', 'dep', 'os', 'its', 'om', 'em', 'oney', 'and', 'then', 'went', 'to', 'eat', 'l', 'unch', 'by', 'ar', 'iver', 'bank', '.']\n",
      "[40, 19963, 83, 20388, 17796, 400, 1042, 273, 768, 11, 10378, 418, 896, 296, 368, 1419, 392, 8524, 19963, 1462, 4098, 75, 3316, 1525, 283, 1428, 17796, 13]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I went t othe bank th ism or ning , dep os its om em oney and then went to eat l unch by ar iver bank .'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_text='I went to the bank this morning, deposit some money and then went to eat lunch by a riverbank.'\n",
    "output_en = tokenizer_en.encode(eng_text)\n",
    "print(output_en.tokens)\n",
    "print(output_en.ids)\n",
    "tokenizer_en.decode(output_en.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "controlling-building",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tokenizers import Tokenizer, models\n",
    "from tokenizers import Tokenizer\n",
    "vocab_f='/workspace/SVdata/gpt2bpe/32k/vocab.json'\n",
    "merge_f='/workspace/SVdata/gpt2bpe/32k/merges.txt'\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.model = models.BPE.from_file(vocab_f, merge_f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "atmospheric-impression",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Har', 'n', 'å', 'gon', 'f', 'under', 'at', 'p', 'å', 'var', 'f', 'ö', 'r', 'man', 'inte', 'f', 'å', 'rin', 'om', 'hu', 'ste', 'peratur', 'en', 'skur', 'vas', 'yn', 'lig', 'ig', 'raf', 'en', '?', 'Ä', 'r', 'det', 'n', 'å', 'gon', 'som', 'fr', 'å', 'gat', 'T', 'her', 'mia', '?', 'Sk', 'ull', 'ed', 'et', 'inte', 'var', 'av', 'ä', 's', 'entligt', 'att', 'k', 'unna', 'koll', 'ah', 'istor', 'iken', 'p', 'å', 'den', 'd', 'å', 'm', 'anska', 'll', 'st', 'ä', 'lla', 'in', 'kur', 'van', '?']\n",
      "[1601, 78, 162, 1978, 70, 1719, 270, 80, 162, 461, 70, 179, 82, 796, 6332, 70, 162, 7211, 283, 12859, 499, 4348, 259, 16750, 10415, 634, 335, 284, 1197, 259, 31, 129, 82, 865, 78, 162, 1978, 747, 742, 162, 6220, 52, 4200, 25375, 31, 1478, 458, 293, 266, 6332, 461, 691, 161, 83, 3962, 287, 75, 6274, 10020, 4747, 1426, 1144, 80, 162, 456, 68, 162, 77, 1028, 271, 279, 161, 1009, 263, 2280, 1409, 31]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Har n å gon f under at p å var f ö r man inte f å rin om hu ste peratur en skur vas yn lig ig raf en ? Ä r det n å gon som fr å gat T her mia ? Sk ull ed et inte var av ä s entligt att k unna koll ah istor iken p å den d å m anska ll st ä lla in kur van ?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_txt=\"Har någon funderat på varför man inte får inomhusteperaturens kurva synlig i grafen? Är det någon som frågat Thermia? Skulle det inte vara väsentligt att kunna kolla historiken på den då man skall ställa in kurvan?\"\n",
    "output = tokenizer.encode(test_txt)\n",
    "print(output.tokens)\n",
    "print(output.ids)\n",
    "tokenizer.decode(output.ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "confidential-infection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a r\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "opponent-sector",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
