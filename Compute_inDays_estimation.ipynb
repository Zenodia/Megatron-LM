{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "super-bench",
   "metadata": {},
   "source": [
    "## The Paper \"Efficient Large-Scale Language Model Training on GPU Clusters\" \n",
    "## has the following equation used in estimating compute needed (in days)\n",
    "\n",
    "![training time estimate](TrainingTimeEstimate.JPG)\n",
    "\n",
    "paper : https://arxiv.org/pdf/2104.04473.pdf\n",
    "\n",
    "- given the following information \n",
    "- T=300*1e+9 dataset size measured in numbers of tokens in the dataset\n",
    "- P=175*1e+9 number of model parameters, for GPT3 \n",
    "- n=640 number of GPUs in the compute cluster\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "![GPT3 variants ](GPT3_all.png)\n",
    "paper : https://arxiv.org/pdf/2005.14165.pdf\n",
    "![Cases](images/cases_jan2021.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eligible-quest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all below are measured with dataset size **300 billion** measured in tokens \n",
      "\n",
      " ----------------------------------------------------------------------------------------\n",
      " language model :gpt3_2.7B with 1.3 Billion number of parameters , it will need 0.6 days to compute \n",
      "\n",
      " ----------------------------------------------------------------------------------------\n",
      " language model :gpt3_6.7B with 2.7 Billion number of parameters , it will need 1.4 days to compute \n",
      "\n",
      " ----------------------------------------------------------------------------------------\n",
      " language model :gpt3_13B with 13 Billion number of parameters , it will need 2.8 days to compute \n",
      "\n",
      " ----------------------------------------------------------------------------------------\n",
      " language model :gpt3_175B with 175 Billion number of parameters , it will need 33.9 days to compute \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "T=300*1e+9 #oftokens in the dataset\n",
    "#P=175*1e+9 # number of model parameters\n",
    "n= 1024 # Berzelius 680 # number of GPUs in the compute cluster\n",
    "\n",
    "def calculate_days_needed(T , P , n ,x):\n",
    "    if x is None:\n",
    "        return '1-2 weeks'\n",
    "    else:\n",
    "        #x=140*1e+12 # TeraFlop/s per GPU\n",
    "        tot=8*T*P\n",
    "        div=n*x\n",
    "        compute_sec=tot/div\n",
    "        #convert compute seconds to days\n",
    "        to_days=round(compute_sec/(3600*24),1)\n",
    "        return to_days\n",
    "\n",
    "GPT3_models_labels=[ 'gpt3_2.7B', 'gpt3_6.7B','gpt3_13B', 'gpt3_175B']\n",
    "GPT3_model_params=[ 2.7*1e+9, 6.7*1e+9 , 13*1e+9, 175*1e+9,1*1e+12 ]\n",
    "GPT3_model_params_str=['1.3 Billion' ,'2.7 Billion', '13 Billion', '175 Billion']\n",
    "#according to the table above\n",
    "GPT3_X=[127*1e+12, 130*1e+12,127*1e+12,140*1e+12 ]\n",
    "print(\"all below are measured with dataset size **300 billion** measured in tokens \\n\")\n",
    "for gpt3_name, gpt3_params, gpt3_param_str, x in zip(GPT3_models_labels,GPT3_model_params,GPT3_model_params_str, GPT3_X ):\n",
    "    days_needed=calculate_days_needed(T,gpt3_params,n,x)\n",
    "    print(\" ----------------------------------------------------------------------------------------\")\n",
    "    print(\" language model :{} with {} number of parameters , it will need {} days to compute \\n\".format(gpt3_name, gpt3_param_str, str(days_needed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "indoor-firmware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Model of **1 Trillion** Parameters, we scale-up the dataset size  to **450 billion** measured in tokens \n",
      "\n",
      "it will take about 83.211 days to compute\n"
     ]
    }
   ],
   "source": [
    "# For Model of 1 Trillion Parameters\n",
    "T=450*1e+9\n",
    "P=1e+12\n",
    "n=3072\n",
    "X=163*1e+12\n",
    "total_compute_seconds=(8*T*P)/(n*X)\n",
    "tot_in_days=round(total_compute_seconds/(3600*24),3)\n",
    "print(\"For Model of **1 Trillion** Parameters, we scale-up the dataset size  to **450 billion** measured in tokens \\n\")\n",
    "print(\"it will take about {} days to compute\".format(str(tot_in_days)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-parent",
   "metadata": {},
   "source": [
    "![the power law](Compute_Datasize_Parameters.JPG)\n",
    "### source : https://arxiv.org/pdf/2001.08361.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
