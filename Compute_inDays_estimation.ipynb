{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deadly-minutes",
   "metadata": {},
   "source": [
    "## The Paper \"Efficient Large-Scale Language Model Training on GPU Clusters\" \n",
    "## has the following equation used in estimating compute needed (in days)\n",
    "\n",
    "![training time estimate](TrainingTimeEstimate.JPG)\n",
    "\n",
    "paper : https://arxiv.org/pdf/2104.04473.pdf\n",
    "\n",
    "- given the following information \n",
    "- T=300*1e+9 dataset size measured in numbers of tokens in the dataset\n",
    "- P=175*1e+9 number of model parameters, for GPT3 \n",
    "- n=640 number of GPUs in the compute cluster\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "![GPT3 variants ](GPT3_all.png)\n",
    "paper : https://arxiv.org/pdf/2005.14165.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "stone-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def superscript(n):\n",
    "    return \"\".join([\"⁰¹²³⁴⁵⁶⁷⁸⁹\"[ord(c)-ord('0')] for c in str(n)]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "tamil-significance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ----------------------------------------------------------------------------------------\n",
      " language model :gpt3_small with 125 Million number of parameters , it will need 0.04 days to compute \n",
      "\n",
      " ----------------------------------------------------------------------------------------\n",
      " language model :gpt3_medium with 350 Million number of parameters , it will need 0.11 days to compute \n",
      "\n",
      " ----------------------------------------------------------------------------------------\n",
      " language model :gpt3_large with 750 Million number of parameters , it will need 0.24 days to compute \n",
      "\n",
      " ----------------------------------------------------------------------------------------\n",
      " language model :gpt3_XL with 1.3 Billion number of parameters , it will need 0.4 days to compute \n",
      "\n",
      " ----------------------------------------------------------------------------------------\n",
      " language model :gpt3_2.7B with 2.7 Billion number of parameters , it will need 0.84 days to compute \n",
      "\n",
      " ----------------------------------------------------------------------------------------\n",
      " language model :gpt3_6.7B with 13 Billion number of parameters , it will need 2.08 days to compute \n",
      "\n",
      " ----------------------------------------------------------------------------------------\n",
      " language model :gpt3_13B with 175 Billion number of parameters , it will need 4.03 days to compute \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "T=300*1e+9 #oftokens in the dataset\n",
    "#P=175*1e+9 # number of model parameters\n",
    "n=640 # number of GPUs in the compute cluster\n",
    "def prettify(x):\n",
    "    s=str(int(x))\n",
    "    l=len(s)\n",
    "    start=s.find('0')\n",
    "    num=str(s[:start])\n",
    "    diff=l-start-1\n",
    "    return num+'x10'+superscript(diff)\n",
    "\n",
    "def calculate_days_needed(T , P , n ):\n",
    "    X=140*1e+12 # TeraFlop/s per GPU\n",
    "    tot=8*T*P\n",
    "    div=n*X\n",
    "    compute_sec=tot/div\n",
    "    #convert compute seconds to days\n",
    "    to_days=round(compute_sec/(3600*24),2)\n",
    "    return to_days\n",
    "\n",
    "GPT3_models_labels=['gpt3_small', 'gpt3_medium', 'gpt3_large', 'gpt3_XL', 'gpt3_2.7B', 'gpt3_6.7B','gpt3_13B', 'gpt3_175B']\n",
    "GPT3_model_params=[125*1e+6, 350*1e+6 , 760*1e+6, 1.3*1e+9 , 2.7*1e+9, 6.7*1e+9 , 13*1e+9, 175*1e+9]\n",
    "GPT_model_params_str=['125 Million','350 Million', '750 Million' ,'1.3 Billion' ,'2.7 Billion', '13 Billion', '175 Billion']\n",
    "\n",
    "for gpt3_name, gpt3_params, gpt3_param_str in zip(GPT3_models_labels,GPT3_model_params,GPT_model_params_str ):\n",
    "    days_needed=calculate_days_needed(T,gpt3_params,n)\n",
    "    print(\" ----------------------------------------------------------------------------------------\")\n",
    "    print(\" language model :{} with {} number of parameters , it will need {} days to compute \\n\".format(gpt3_name, gpt3_param_str, str(days_needed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-claim",
   "metadata": {},
   "source": [
    "![the power law](Compute_Datasize_Parameters.JPG)\n",
    "### source : https://arxiv.org/pdf/2001.08361.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
