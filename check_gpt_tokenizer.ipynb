{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading gpt2 tokenizer\n",
      "[70, 103, 118, 287, 194, 163, 116, 287, 104, 194, 181, 116, 117, 103, 112, 46, 287, 120, 107, 287, 105, 194, 164, 116, 287, 106, 103, 111, 287, 112, 119]\n"
     ]
    }
   ],
   "source": [
    "from megatron.tokenizer.tokenizer import build_debug\n",
    "#sample_doc='邻近皇宫萨纳雷斯河，穆斯林称为（阿拉伯语:المجريط，「水的来源」）。'\n",
    "raw_text='Det är försen, vi går hem nu'\n",
    "vocab_f='/workspace/SVdata/bpeHash/32k/vocab.json'\n",
    "merge_f='/workspace/SVdata/bpeHash/32k/merges.txt'\n",
    "tokenizer=build_debug(vocab_f,merge_f, 'gpt')\n",
    "token_ids=tokenizer.tokenize(raw_text)\n",
    "print(token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in gpt2tokenizer decode function  DetĠÃ¤rĠfÃ¶rsen,ĠviĠgÃ¥rĠhemĠnu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Det är försen, vi går hem nu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.detokenize(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "如果 34988\n",
      "变动 38916\n",
      "较大 36463\n",
      "， 32471\n",
      "随时 36903\n",
      "关注 35192\n",
      "最新 35754\n",
      "思路 37054\n",
      "。 5163\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "vocab_id_list=[v for (k,v) in tokenizer.vocab.items()] \n",
    "vocab_id_to_token_dict=tokenizer.inv_vocab\n",
    "masked_lm_prob=0.15\n",
    "cls_id=tokenizer.vocab['[CLS]']\n",
    "sep_id=tokenizer.vocab['[SEP]']\n",
    "mask_id=tokenizer.vocab['[MASK]']\n",
    "max_predictions_per_seq=512\n",
    "np_rng=np.random.default_rng(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a98ea4cc2da9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvocab_id_list2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mvocab_id_to_token_dict2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv_vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmasked_lm_pro2b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer2' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "vocab_id_list2=[v for (k,v) in tokenizer2.vocab.items()] \n",
    "vocab_id_to_token_dict2=tokenizer.inv_vocab\n",
    "masked_lm_pro2b=0.15\n",
    "cls_id2=tokenizer2.vocab['[CLS]']\n",
    "sep_id2=tokenizer2.vocab['[SEP]']\n",
    "mask_id2=tokenizer2.vocab['[MASK]']\n",
    "max_predictions_per_seq2=512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "vocab_id_list3=[v for (k,v) in tokenizer3.vocab.items()] \n",
    "vocab_id_to_token_dict3=tokenizer3.inv_vocab\n",
    "masked_lm_pro2b=0.15\n",
    "cls_id2=tokenizer3.vocab['[CLS]']\n",
    "sep_id2=tokenizer3.vocab['[SEP]']\n",
    "mask_id2=tokenizer3.vocab['[MASK]']\n",
    "max_predictions_per_seq3=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random as np_rng\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from megatron.data.dataset_utils import create_masked_lm_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tokens, masked_lm_positions, masked_lm_labels, token_boundary= create_masked_lm_predictions(token_ids,\n",
    "                                 vocab_id_list, vocab_id_to_token_dict,\n",
    "                                 masked_lm_prob,\n",
    "                                 cls_id, sep_id, mask_id,\n",
    "                                 max_predictions_per_seq,\n",
    "                                 np_rng,\n",
    "                                 3,\n",
    "                                 True,\n",
    "                                 False,\n",
    "                                 False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([70,\n",
       "  5,\n",
       "  118,\n",
       "  287,\n",
       "  194,\n",
       "  163,\n",
       "  116,\n",
       "  287,\n",
       "  104,\n",
       "  194,\n",
       "  181,\n",
       "  5,\n",
       "  5,\n",
       "  103,\n",
       "  5,\n",
       "  5,\n",
       "  287,\n",
       "  120,\n",
       "  107,\n",
       "  287,\n",
       "  105,\n",
       "  194,\n",
       "  164,\n",
       "  116,\n",
       "  287,\n",
       "  106,\n",
       "  103,\n",
       "  111,\n",
       "  287,\n",
       "  112,\n",
       "  119],\n",
       " [1, 11, 12, 14, 15],\n",
       " [103, 116, 117, 112, 46],\n",
       " [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokens, masked_lm_positions, masked_lm_labels, token_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tokens2, masked_lm_positions2,masked_lm_labels2, token_boundary2= create_masked_lm_predictions(token_ids2,\n",
    "                                 vocab_id_list2, vocab_id_to_token_dict2,\n",
    "                                 masked_lm_prob,\n",
    "                                 cls_id2, sep_id2, mask_id2,\n",
    "                                 max_predictions_per_seq2,\n",
    "                                 np_rng,\n",
    "                                 3,\n",
    "                                 True,\n",
    "                                 False,\n",
    "                                 False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tokens2, masked_lm_positions2,masked_lm_labels2, token_boundary2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
