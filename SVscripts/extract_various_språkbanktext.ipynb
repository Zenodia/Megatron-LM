{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "opening-amateur",
   "metadata": {},
   "source": [
    "## wget sb_corpus_reader.py from https://github.com/spraakbanken/sb-nltk-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "classified-string",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['familjeliv-allmanna-ekonomi.xml', 'sv_train.txt', 'gp2013.xml', '32k', 'json', 'familjeliv-foralder.xml', 'storsuc.xml', '.ipynb_checkpoints', 'talbanken.xml', 'out', 'wikipedia-sv.xml', 'sweacsam.xml']\n",
      "fname:  familjeliv_allmanna_ekonomi.txt\n",
      "fname:  gp2013.txt\n",
      "fname:  familjeliv_foralder.txt\n",
      "fname:  storsuc.txt\n",
      "fname:  talbanken.txt\n",
      "fname:  wikipedia_sv.txt\n",
      "fname:  sweacsam.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sb_corpus_reader import SBCorpusReader\n",
    "import random\n",
    "out_path='/workspace/SVdata/raw/out/'\n",
    "done_xml=['familjeliv-allmanna-ekonomi.xml','gp2013.xml']\n",
    "xml_files=os.listdir('raw')\n",
    "print(xml_files)\n",
    "for xml_f in xml_files:\n",
    "    if xml_f.endswith('.xml'):\n",
    "        \n",
    "        fname='_'.join(xml_f.split('.')[0].split('-'))+'.txt'  \n",
    "        print(\"fname: \",fname)\n",
    "def write2csv(out_path, fname, sents):\n",
    "    f=open(out_path+fname,'a')\n",
    "    for s in sents:\n",
    "        if len(s)>=13:\n",
    "            s_text=' '.join(s)\n",
    "            f.write(s_text+'\\n')    \n",
    "    f.close()\n",
    "    print(\"finish processing \",fname)\n",
    "\n",
    "for xml_f in xml_files:\n",
    "    if xml_f.endswith('.xml') and xml_f not in done_xml:\n",
    "        corpus = SBCorpusReader('./raw/'+xml_f)\n",
    "        sents=corpus.sents()\n",
    "        print(sents[:2])\n",
    "        #n=len(sents)\n",
    "        #rn=random.randint(0,n-1)\n",
    "        #print(\"a random sample of sentence : \\n\".format(' '.join(sents[rn])))\n",
    "        fname='_'.join(xml_f.split('.')[0].split('-'))+'.txt'  \n",
    "        print(\"write to : \",fname)\n",
    "        write2csv(out_path,fname,sents)\n",
    "        print('-----'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "available-blackjack",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "about-smooth",
   "metadata": {},
   "source": [
    "## extracting text per spr√•kbank data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "advance-fundamentals",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-543007ecd57a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSBCorpusReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./raw/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mxml_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0msents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;31m#n=len(sents)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#rn=random.randint(0,n-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/nltk/corpus/reader/util.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m             \u001b[0;31m# Check if it's in the cache.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/nltk/internals.py\u001b[0m in \u001b[0;36mslice_bounds\u001b[0;34m(sequence, slice_obj, allow_step)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m             \u001b[0msequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstop\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m             \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/nltk/corpus/reader/util.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;31m# Use iterate_from to extract it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"index out of range\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/nltk/corpus/reader/util.py\u001b[0m in \u001b[0;36miterate_from\u001b[0;34m(self, start_tok)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_toknum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoknum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_blocknum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m             assert isinstance(tokens, (tuple, list, AbstractLazySequence)), (\n\u001b[1;32m    298\u001b[0m                 \u001b[0;34m\"block reader %s() should return list or tuple.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/SVdata/sb_corpus_reader.py\u001b[0m in \u001b[0;36m_read_sent_block\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_sent_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0msents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_tagged_sent_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mws\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/SVdata/sb_corpus_reader.py\u001b[0m in \u001b[0;36m_read_tagged_sent_block\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSENTS_PER_BLOCK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1153\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m         \u001b[0;34m\"\"\"Return the next decoded line from the underlying stream.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1146\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sb_corpus_reader import SBCorpusReader\n",
    "import random\n",
    "out_path='/workspace/SVdata/raw/out/'\n",
    "done_xml=['familjeliv-allmanna-ekonomi.xml','gp2013.xml']\n",
    "for xml_f in xml_files:\n",
    "    if xml_f.endswith('.xml') and xml_f not in done_xml:\n",
    "        corpus = SBCorpusReader('./raw/'+xml_f)\n",
    "        sents=corpus.sents()\n",
    "        print(sents[:2])\n",
    "        #n=len(sents)\n",
    "        #rn=random.randint(0,n-1)\n",
    "        #print(\"a random sample of sentence : \\n\".format(' '.join(sents[rn])))\n",
    "        fname='_'.join(xml_f.split('.')[0].split('-'))+'.txt'  \n",
    "        print(\"write to : \",fname)\n",
    "        write2csv(out_path,fname,sents)\n",
    "        print('-----'*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-trinidad",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "for s in sents:\n",
    "    if k<=2:\n",
    "        if len(s)>=4:            \n",
    "            s_text=' '.join(s)\n",
    "            print(s_text)\n",
    "            k+=1\n",
    "    else:break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-firewall",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path='/workspace/SVdata/raw/out/'\n",
    "fname='familjeliv_allmanna_ekonomi.csv'\n",
    "def write2csv(out_path, fname, sents):\n",
    "    f=open(out_path+fname,'a')\n",
    "    for s in sents:\n",
    "        if len(s)>=4:\n",
    "            s_text=' '.join(s)\n",
    "            f.write(s_text+'\\n')    \n",
    "    f.close()\n",
    "    print(\"finish processing \",fname)\n",
    "write2csv(out_path,fname,sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-assurance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-rebecca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-knight",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "class CustomLanguageVars(nltk.tokenize.punkt.PunktLanguageVars):\n",
    "\n",
    "    _period_context_fmt = r\"\"\"\n",
    "        \\S*                          # some word material\n",
    "        %(SentEndChars)s             # a potential sentence ending\n",
    "        \\s*                       #  <-- THIS is what I changed\n",
    "        (?=(?P<after_tok>\n",
    "            %(NonWord)s              # either other punctuation\n",
    "            |\n",
    "            (?P<next_tok>\\S+)     #  <-- Normally you would have \\s+ here\n",
    "        ))\"\"\"\n",
    "\n",
    "class IdentitySplitter(object):\n",
    "    def tokenize(self, *text):\n",
    "        return text\n",
    "splitter = nltk.load(\"tokenizers/punkt/english.pickle\")\n",
    "split=nltk.tokenize.punkt.PunktSentenceTokenizer(\n",
    "                    train_text = splitter._params,\n",
    "                    lang_vars = CustomLanguageVars())\n",
    "\n",
    "s='P√• vilket s√§tt skulle det bli sv√•rare , kan du f√∂rklara ur en socarbetares synvinkel ?Haha , inte med IQ badboll heller ... Jag tror inte att hon menar att s√§ttet som hon skriver p√• p√•verkar , utan snarare att s√§ttet p√•visar problem som g√∂r att ...'\n",
    "o=split.tokenize(s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentences=[]\n",
    "i=0\n",
    "f=open('./raw/familijeliv_allmanna_ekonomi.csv','a')\n",
    "for node in root.iter('thread'):\n",
    "    print(\"-----------------------------------------thread {} ------------------------------------- \\n\\n\\n\".format(str(i)))\n",
    "    temp=[]\n",
    "    for x in node.iter('sentence'):\n",
    "        sent=fetch_words(x)\n",
    "        out=split.tokenize(sent)\n",
    "        if len(out)>0:\n",
    "            temp.append(sent)            \n",
    "    print(''.join(temp))\n",
    "    i+=1\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-journal",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-spanish",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-treasury",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-degree",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for page in root.findall('w'):\n",
    "    if i<=10:\n",
    "        print(\"Title: \", page.text)\n",
    "        i+=1\n",
    "    else:break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
