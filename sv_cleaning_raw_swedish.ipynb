{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) use sed -r '/^\\s*$/d' sv.txt > nonempty_sv.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) use split4parts to split nonempty_sv.txt to 4 parts for processing due to PC host mem constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3) run thru the following SVfilter.py 4 times to process the data to the cleaned folder \n",
    "#### example syntax \n",
    "#### python SVfilter.py --input_dir SV_CC100_part_aaad --output_dir ./cleaned/SV_CC100_Part4.txt --num_cpu 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile SVfilter.py\n",
    "import sys\n",
    "import multiprocessing\n",
    "import os\n",
    "from multiprocessing import Pool,cpu_count\n",
    "import nltk\n",
    "sent_tokenizer = nltk.tokenize.PunktSentenceTokenizer()\n",
    "import argparse\n",
    "\n",
    "def cut(sentence):\n",
    "    sents=sent_tokenizer.tokenize(sentence)\n",
    "    sents=[s for s in sents if len(s)>1]\n",
    "    if len(sents)>2: # we filter only more than 2 sentences in each doc\n",
    "        return ''.join(sents)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def main(args):\n",
    "    num_cpus_to_use=int(args.num_cpu)\n",
    "    lines=[]\n",
    "    with open(args.input_dir, 'r', errors='ignore') as f:\n",
    "        for k, line in enumerate(f, 1):\n",
    "            line=line.replace('~','')\n",
    "            line=line.replace('…','')\n",
    "            line=line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            else:\n",
    "                if len(line)>0 and line not in ['\\n','','\\r\\n','\\t',' ']:\n",
    "                    lines.append(line)\n",
    "                    k+=1\n",
    "    print(\"Finish reading the file with number of non_empty lines \", str(k))\n",
    "    # 创建进程池\n",
    "    pool = Pool(num_cpus_to_use)\n",
    "    data = pool.imap(cut,lines, 512)\n",
    "    i=0\n",
    "    outfile = open(args.output_dir, 'a')\n",
    "    for i,dat in enumerate(data):\n",
    "        if len(dat) >0:\n",
    "            #print(i,dat,type(dat))\n",
    "            outfile.write(dat+'\\n')\n",
    "        if i%1000000==0:\n",
    "            print(\"processed {} million lines ...\\n\".format(str(i/1000000)))\n",
    "            print(\"sample dat at i\",i, dat)\n",
    "    print(\"processing finished !\")\n",
    "    f.close()\n",
    "    outfile.close()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input_dir', default=None, type=str,  help='input file path')\n",
    "    parser.add_argument('--output_dir', default=None, type=str, \n",
    "                        help='output file path')\n",
    "    parser.add_argument('--num_cpu', default=48, type=int, \n",
    "                        help='number of cpus used for multiprocesing')\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text : \n",
      " \n",
      "Efter en intensiv helg med ridlektionsjobb och sånguppträdande, mycket intryck och människor tar jag en lugn dag hemma idag. Blir ju alltid lite dagen-efter efter sånt här, huvudvärk, kan typ sova ett dygn, yr och muskelvärk.Helgen har trots sin intensitet varit väldigt väldigt fin.\n",
      "Fått umgås med familjen och min älskade sambo.En fin vistelse på hotell osv.Milla har fått vara hos våra vänner Malin och Theo och levt livet - där bor nämligen också hennes bästishund Twix.Min hjärna blir ju extremt trött av intryck så tycker lite synd om stackars Martin när det blir såna här intensiva helger.Jag blir liksom helt slut efteråt mentalt och fysiskt.Som ett skal,\n",
      " och grubblig och ledsen.Hjärnan kokar över.Men känns skönt att det blir bättre och bättre hela tiden.\n",
      "------------------------------------------------------------\n",
      "split into sentence : \n",
      "\n",
      "sentence 0 : Efter en intensiv helg med ridlektionsjobb och sånguppträdande, mycket intryck och människor tar jag en lugn dag hemma idag. \n",
      "\n",
      "sentence 1 : Blir ju alltid lite dagen-efter efter sånt här, huvudvärk, kan typ sova ett dygn, yr och muskelvärk.Helgen har trots sin intensitet varit väldigt väldigt fin. \n",
      "\n",
      "sentence 2 : Fått umgås med familjen och min älskade sambo.En fin vistelse på hotell osv.Milla har fått vara hos våra vänner Malin och Theo och levt livet - där bor nämligen också hennes bästishund Twix.Min hjärna blir ju extremt trött av intryck så tycker lite synd om stackars Martin när det blir såna här intensiva helger.Jag blir liksom helt slut efteråt mentalt och fysiskt.Som ett skal,\n",
      " och grubblig och ledsen.Hjärnan kokar över.Men känns skönt att det blir bättre och bättre hela tiden. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text=\"Efter en intensiv helg med ridlektionsjobb och sånguppträdande, mycket intryck och människor tar jag en lugn dag hemma idag. Blir ju alltid lite dagen-efter efter sånt här, huvudvärk, kan typ sova ett dygn, yr och muskelvärk.Helgen har trots sin intensitet varit väldigt väldigt fin.\\nFått umgås med familjen och min älskade sambo.En fin vistelse på hotell osv.Milla har fått vara hos våra vänner Malin och Theo och levt livet - där bor nämligen också hennes bästishund Twix.Min hjärna blir ju extremt trött av intryck så tycker lite synd om stackars Martin när det blir såna här intensiva helger.Jag blir liksom helt slut efteråt mentalt och fysiskt.Som ett skal,\\n och grubblig och ledsen.Hjärnan kokar över.Men känns skönt att det blir bättre och bättre hela tiden.\"\n",
    "i=0\n",
    "print(\"original text : \\n \")\n",
    "print(text)\n",
    "print(\"---\"*20)\n",
    "sents=sent_tokenize(text)\n",
    "print(\"split into sentence : \\n\")\n",
    "for sent in sents:\n",
    "    print(\"sentence {} : {} \\n\".format(str(i), sent))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (4) run the train tokenizer script for the following pretrained tokenizer\n",
    "### BPE with vocab size 16k/32k and 48k, \n",
    "#### note the 16k BPE does not do a good job, seperting the tokenize into individual characters ... \n",
    "### WordPiece with vocab size 16k/32k/48k \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (5) run preprocessing script for both GPT ( original ) and Bert ( preprocessSVdata.py ) and use 32k to start with \n",
    "### do NOT use the multiprocessing, it will blow up the system mem and kill job or has broken pipe error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from megatron.tokenizer.tokenizer import build_debug\n",
    "vocab_f='/workspace/SVdata/bpe/32k/vocab.json'\n",
    "merge_f='/workspace/SVdata/bpe/32k/merges.txt'\n",
    "sv_tokenizer=build_debug(vocab_f,merge_f, tokenizer_type='bpe')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence :Efter en intensiv helg med ridlektionsjobb och sånguppträdande, mycket intryck och människor tar jag en lugn dag hemma idag. \n",
      " \n",
      "id : 17371 , tok:Min \n",
      "\n",
      "id : 16439 , tok:hjärna \n",
      "\n",
      "id : 25936 , tok:blir \n",
      "\n",
      "id : 18870 , tok:ju \n",
      "\n",
      "id : 16475 , tok:extrem \n",
      "\n",
      "id : 19897 , tok:tt \n",
      "\n",
      "id : 16540 , tok:rött \n",
      "\n",
      "id : 21782 , tok:av \n",
      "\n",
      "id : 17100 , tok:intryck \n",
      "\n",
      "id : 16455 , tok:så \n",
      "\n",
      "id : 19745 , tok:tycker \n",
      "\n",
      "id : 16531 , tok:lite \n",
      "\n",
      "id : 25729 , tok:syn \n",
      "\n",
      "id : 46 , tok:dom \n",
      "\n",
      "id : 16719 , tok:stackars \n",
      "\n",
      "id : 23500 , tok:Mart \n",
      "\n",
      "id : 16455 , tok:inn \n",
      "\n",
      "id : 17072 , tok:är \n",
      "\n",
      "id : 18481 , tok:det \n",
      "\n",
      "id : 16440 , tok:blir \n",
      "\n",
      "id : 16506 , tok:såna \n",
      "\n",
      "id : 16439 , tok:här \n",
      "\n",
      "id : 18902 , tok:intensiva \n",
      "\n",
      "id : 16549 , tok:helger \n",
      "\n",
      "id : 17558 , tok:. \n",
      "\n",
      "id : 17113 , tok:Jag \n",
      "\n",
      "id : 48 , tok:blir \n",
      "\n",
      "------------------------------\n",
      "sentence :Blir ju alltid lite dagen-efter efter sånt här, huvudvärk, kan typ sova ett dygn, yr och muskelvärk.Helgen har trots sin intensitet varit väldigt väldigt fin. \n",
      " \n",
      "id : 22323 , tok:[UNK] \n",
      "\n",
      "id : 16539 , tok:[UNK] \n",
      "\n",
      "id : 17157 , tok:[UNK] \n",
      "\n",
      "id : 17762 , tok:[UNK] \n",
      "\n",
      "id : 17634 , tok:[UNK] \n",
      "\n",
      "id : 16439 , tok:[UNK] \n",
      "\n",
      "id : 47 , tok:[UNK] \n",
      "\n",
      "id : 16644 , tok:[UNK] \n",
      "\n",
      "id : 16644 , tok:[UNK] \n",
      "\n",
      "id : 20928 , tok:[UNK] \n",
      "\n",
      "id : 16597 , tok:[UNK] \n",
      "\n",
      "id : 46 , tok:[UNK] \n",
      "\n",
      "id : 28734 , tok:[UNK] \n",
      "\n",
      "id : 46 , tok:[UNK] \n",
      "\n",
      "id : 16510 , tok:[UNK] \n",
      "\n",
      "id : 16814 , tok:[UNK] \n",
      "\n",
      "id : 16958 , tok:[UNK] \n",
      "\n",
      "id : 31504 , tok:[UNK] \n",
      "\n",
      "id : 16511 , tok:[UNK] \n",
      "\n",
      "id : 23772 , tok:[UNK] \n",
      "\n",
      "id : 46 , tok:[UNK] \n",
      "\n",
      "id : 16788 , tok:[UNK] \n",
      "\n",
      "id : 16455 , tok:[UNK] \n",
      "\n",
      "id : 24820 , tok:[UNK] \n",
      "\n",
      "id : 22929 , tok:[UNK] \n",
      "\n",
      "id : 48 , tok:[UNK] \n",
      "\n",
      "id : 17590 , tok:[UNK] \n",
      "\n",
      "id : 16564 , tok:[UNK] \n",
      "\n",
      "id : 16501 , tok:[UNK] \n",
      "\n",
      "id : 16807 , tok:[UNK] \n",
      "\n",
      "id : 118 , tok:[UNK] \n",
      "\n",
      "id : 28402 , tok:[UNK] \n",
      "\n",
      "id : 20977 , tok:[UNK] \n",
      "\n",
      "id : 16686 , tok:[UNK] \n",
      "\n",
      "id : 17027 , tok:[UNK] \n",
      "\n",
      "id : 17292 , tok:[UNK] \n",
      "\n",
      "id : 17292 , tok:[UNK] \n",
      "\n",
      "id : 16724 , tok:[UNK] \n",
      "\n",
      "id : 48 , tok:[UNK] \n",
      "\n",
      "------------------------------\n",
      "sentence :Fått umgås med familjen och min älskade sambo.En fin vistelse på hotell osv.Milla har fått vara hos våra vänner Malin och Theo och levt livet - där bor nämligen också hennes bästishund Twix. \n",
      " \n",
      "id : 19424 , tok:[UNK] \n",
      "\n",
      "id : 16463 , tok:[UNK] \n",
      "\n",
      "id : 24203 , tok:[UNK] \n",
      "\n",
      "id : 16475 , tok:[UNK] \n",
      "\n",
      "id : 19087 , tok:[UNK] \n",
      "\n",
      "id : 16455 , tok:[UNK] \n",
      "\n",
      "id : 16569 , tok:[UNK] \n",
      "\n",
      "id : 22000 , tok:[UNK] \n",
      "\n",
      "id : 24582 , tok:[UNK] \n",
      "\n",
      "id : 48 , tok:[UNK] \n",
      "\n",
      "id : 16690 , tok:[UNK] \n",
      "\n",
      "id : 16724 , tok:[UNK] \n",
      "\n",
      "id : 22467 , tok:[UNK] \n",
      "\n",
      "id : 16468 , tok:[UNK] \n",
      "\n",
      "id : 17227 , tok:[UNK] \n",
      "\n",
      "id : 20722 , tok:[UNK] \n",
      "\n",
      "id : 48 , tok:[UNK] \n",
      "\n",
      "id : 79 , tok:[UNK] \n",
      "\n",
      "id : 18182 , tok:[UNK] \n",
      "\n",
      "id : 16501 , tok:[UNK] \n",
      "\n",
      "id : 17302 , tok:[UNK] \n",
      "\n",
      "id : 16649 , tok:[UNK] \n",
      "\n",
      "id : 17034 , tok:[UNK] \n",
      "\n",
      "id : 16973 , tok:[UNK] \n",
      "\n",
      "id : 18070 , tok:[UNK] \n",
      "\n",
      "id : 24240 , tok:[UNK] \n",
      "\n",
      "id : 16455 , tok:[UNK] \n",
      "\n",
      "id : 17890 , tok:[UNK] \n",
      "\n",
      "id : 113 , tok:[UNK] \n",
      "\n",
      "id : 16455 , tok:[UNK] \n",
      "\n",
      "id : 27196 , tok:[UNK] \n",
      "\n",
      "id : 17592 , tok:[UNK] \n",
      "\n",
      "id : 47 , tok:[UNK] \n",
      "\n",
      "id : 16631 , tok:[UNK] \n",
      "\n",
      "id : 16946 , tok:[UNK] \n",
      "\n",
      "id : 19448 , tok:[UNK] \n",
      "\n",
      "id : 16725 , tok:[UNK] \n",
      "\n",
      "id : 17919 , tok:[UNK] \n",
      "\n",
      "id : 18062 , tok:[UNK] \n",
      "\n",
      "id : 16652 , tok:[UNK] \n",
      "\n",
      "id : 17702 , tok:[UNK] \n",
      "\n",
      "id : 19352 , tok:[UNK] \n",
      "\n",
      "id : 107 , tok:[UNK] \n",
      "\n",
      "id : 122 , tok:[UNK] \n",
      "\n",
      "id : 48 , tok:[UNK] \n",
      "\n",
      "------------------------------\n",
      "sentence :Min hjärna blir ju extremt trött av intryck så tycker lite synd om stackars Martin när det blir såna här intensiva helger.Jag blir liksom helt slut efteråt mentalt och fysiskt.Som ett skal,\n",
      " och grubblig och ledsen.Hjärnan kokar över.Men känns skönt att det blir bättre och bättre hela tiden. \n",
      " \n",
      "id : 17529 , tok:[UNK] \n",
      "\n",
      "id : 27445 , tok:[UNK] \n",
      "\n",
      "id : 16792 , tok:[UNK] \n",
      "\n",
      "id : 16539 , tok:[UNK] \n",
      "\n",
      "id : 29214 , tok:[UNK] \n",
      "\n",
      "id : 16463 , tok:[UNK] \n",
      "\n",
      "id : 21306 , tok:[UNK] \n",
      "\n",
      "id : 16474 , tok:[UNK] \n",
      "\n",
      "id : 23500 , tok:[UNK] \n",
      "\n",
      "id : 16502 , tok:[UNK] \n",
      "\n",
      "id : 17215 , tok:[UNK] \n",
      "\n",
      "id : 16747 , tok:[UNK] \n",
      "\n",
      "id : 17255 , tok:[UNK] \n",
      "\n",
      "id : 16833 , tok:[UNK] \n",
      "\n",
      "id : 30577 , tok:[UNK] \n",
      "\n",
      "id : 20257 , tok:[UNK] \n",
      "\n",
      "id : 16537 , tok:[UNK] \n",
      "\n",
      "id : 16449 , tok:[UNK] \n",
      "\n",
      "id : 16470 , tok:[UNK] \n",
      "\n",
      "id : 16792 , tok:[UNK] \n",
      "\n",
      "id : 25780 , tok:[UNK] \n",
      "\n",
      "id : 16597 , tok:[UNK] \n",
      "\n",
      "id : 31343 , tok:[UNK] \n",
      "\n",
      "id : 28859 , tok:[UNK] \n",
      "\n",
      "id : 48 , tok:[UNK] \n",
      "\n",
      "id : 16620 , tok:[UNK] \n",
      "\n",
      "id : 16792 , tok:[UNK] \n",
      "\n",
      "id : 18900 , tok:[UNK] \n",
      "\n",
      "id : 16916 , tok:[UNK] \n",
      "\n",
      "id : 16546 , tok:[UNK] \n",
      "\n",
      "id : 17473 , tok:[UNK] \n",
      "\n",
      "id : 16831 , tok:[UNK] \n",
      "\n",
      "id : 16764 , tok:[UNK] \n",
      "\n",
      "id : 16523 , tok:[UNK] \n",
      "\n",
      "id : 18359 , tok:[UNK] \n",
      "\n",
      "id : 16455 , tok:[UNK] \n",
      "\n",
      "id : 25742 , tok:[UNK] \n",
      "\n",
      "id : 48 , tok:[UNK] \n",
      "\n",
      "id : 17338 , tok:[UNK] \n",
      "\n",
      "id : 16511 , tok:[UNK] \n",
      "\n",
      "id : 19403 , tok:[UNK] \n",
      "\n",
      "id : 46 , tok:[UNK] \n",
      "\n",
      "id : 16455 , tok:[UNK] \n",
      "\n",
      "id : 23050 , tok:[UNK] \n",
      "\n",
      "id : 16616 , tok:[UNK] \n",
      "\n",
      "id : 16491 , tok:[UNK] \n",
      "\n",
      "id : 16455 , tok:[UNK] \n",
      "\n",
      "id : 25273 , tok:[UNK] \n",
      "\n",
      "id : 48 , tok:[UNK] \n",
      "\n",
      "id : 23349 , tok:[UNK] \n",
      "\n",
      "id : 18328 , tok:[UNK] \n",
      "\n",
      "id : 19922 , tok:[UNK] \n",
      "\n",
      "id : 16440 , tok:[UNK] \n",
      "\n",
      "id : 16589 , tok:[UNK] \n",
      "\n",
      "id : 48 , tok:[UNK] \n",
      "\n",
      "id : 16783 , tok:[UNK] \n",
      "\n",
      "id : 17820 , tok:[UNK] \n",
      "\n",
      "id : 19049 , tok:[UNK] \n",
      "\n",
      "id : 16458 , tok:[UNK] \n",
      "\n",
      "id : 16470 , tok:[UNK] \n",
      "\n",
      "id : 16792 , tok:[UNK] \n",
      "\n",
      "id : 17191 , tok:[UNK] \n",
      "\n",
      "id : 16455 , tok:[UNK] \n",
      "\n",
      "id : 17191 , tok:[UNK] \n",
      "\n",
      "id : 16621 , tok:[UNK] \n",
      "\n",
      "id : 16522 , tok:[UNK] \n",
      "\n",
      "id : 16482 , tok:[UNK] \n",
      "\n",
      "id : 48 , tok:[UNK] \n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sent in sents:\n",
    "    ids=sv_tokenizer.tokenize(sent)\n",
    "    toks=sv_tokenizer.decode_token_ids(toks)\n",
    "    print(\"sentence :{} \\n \".format(sent))\n",
    "    for tok, idx in zip(toks,ids):\n",
    "        print(\"id : {} , tok:{} \\n\".format(idx, tok))\n",
    "    print(\"---\"*10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
